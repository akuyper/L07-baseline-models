---
title: "L07 Baseline Models"
subtitle: "Data Science 2 with R (STAT 301-2)"
author: "YOUR NAME"
date: today

format:
  html:
    toc: true
    toc-depth: 4
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  warning: false
  
from: markdown+emoji

reference-location: margin
citation-location: margin
---

::: {.callout-tip icon="false"}
## Github Repo Link

To link to your github **repo**sitory, appropriately edit the example link below. Meaning replace `https://your-github-repo-url` with your github repo url. Suggest verifying the link works before submitting.

[https://your-github-repo-url](https://your-github-repo-url)
:::

## Overview

The goal of this lab is to introduce the concept of baseline models and their role in helping us determine if taking the time to build a detailed predictive model is worth while. Baseline models can also play an important role in setting benchmarks.  

## Exercises

We will be re-visiting the abalone (Exercise 1) and titanic (Exercise 2) datasets used in Lab 04 Judging Models. 

For each exercise we have completed the initial setup, data preprocessing/recipes, defining workflows, and implementation of model tuning for various candidate model types. The details of which are provided in each exercise. While the R scripts are also included, they are placeholders. The majority of the code has been removed from them, but enough details have been provided in this document and in the scripts such that the missing code could be re-created --- **students are not required to do this**. 

The main objective is to define and train baseline models to appropriately compare to the results of the other models.

### Exercise 1

For this exercise, we will be working with the abalone dataset^[UCI Machine Learning repository ([see website](http://archive.ics.uci.edu/ml/datasets/Abalone))]. The dataset consists of 4,177 observations of abalone in Tasmania. 

::: {.callout-note icon="false"}
## Prediction goal

Our goal is to predict abalone age, which is calculated as the number of rings plus 1.5. 
:::

#### Task 1

::: {.callout-important collapse="true" icon="false"}

## Details for initial setup, model/workflow specs, & preprocessing

When reading in the data we created the target variable `age` by adding 1.5 to the number of rings for each observation. We also change the variable `type` to a factor. After determining there were no issues with the target variable and there were no missingness issues with predictor variables, we proceeded to splitting the data and constructing resamples.

We implemented an 80-20 training-test split using stratified sampling (stratified by target variable with 4 strata). The resamples were constructed by taking the training dataset and applying repeated V-fold cross-validation (10 folds, 5 repeats) with stratification on the target variable with 4 strata.

The following model types were specified with a plan for tuning hyperparameters using a regular grid:

- Linear model (`lm` engine)
- Elastic net model (`glmnet` engine)
    - Mixture was explored over $[0,1]$ range with 11 levels
    - Penalty was explored over $[-3,0]$ range with 11 levels (on log-10 scale)
- K-nearest neighbors model (`kknn` engine)
    - Neighbors was explored over $[1,25]$ with 7 levels
- Random forest (`ranger` engine)
    - Number of trees set to 1,000
    - Number of randomly selected predictors to split on was explored over $[1,6]$ with 6 levels
    - Minimum number of data points in a node for splitting was explored over $[2,40]$ with 5 levels
- Boosted tree (`xgboost` engine)
    - Number of trees was explored over $[100, 1000]$ with 4 levels
    - Number of randomly selected predictors to split on was explored over $[1,6]$ with 6 levels
    - Minimum number of data points in a node for splitting was explored over $[2,40]$ with 4 levels
    - Learning rate was explored over $[-5,-0.2]$ with 4 levels (on log-10 scale)
    
The preprocessing/feature engineering for these models can be observed in the `2_recipes_ex1.R` script. Reviewing will help to fully understand the preprocessing steps used.

The linear and elastic net models used the preprocessing stored in `abalone_recipe_lm`. We take care to remove appropriate variables (`rings` and variables with zero variance), dummy code any factor variables, add complexity by including some interactions, and standardize all numerical variables.

The k-nearest neighbors, random forest, and boosted tree models used the preprocessing stored in `abalone_recipe_tree`. We take care to remove appropriate variables (`rings` and any variables with zero variance), one-hot encode factor variables, and standardize all numerical variables.

Before any training of models occurs we decided the RMSE would be the metric by which we would compare models.

:::

##### Part A

On each fold, about how much data is used for training the model and about how much is used to produce an assessment estimate? How many assessment estimates are being averaged to produce one estimate per model? 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

##### Part B

It is always useful to have a sense of how many models where included in the model comparison stage --- especially when considering computational resources. Having it broken down by type lets us know which model types were considered and the number of submodels hint at how thorough each model type was explored. Fill in the missing values in @tbl-mod-totals-reg.


| Model Type          | Number of models | Total number of trainings^[Number of times a training process must be completed] |
|---------------------|-----------------:|--------------------------:|
| Linear model        |                  |                           |
| Elastic net         |                  |                           |
| K-nearest neighbors |                  |                           |
| Random forest       |                  |                           |
| Boosted tree        |                  |                           |
| **Total**           |                  |                           |

: Model Training Totals for Regression Problem {#tbl-mod-totals-reg .striped .hover}

##### Part C

Suppose each linear, elastic net, and nearest neighbor model takes 2 seconds to fit, while each random forest and boosted tree model takes 6 seconds. About how many minutes^[Report in additional unit(s) of time, if it would be more useful/impactful.] would it take to train all the models given the breakdown in @tbl-mod-totals-reg --- assuming we fit one model after the other (meaning fit sequentially)? 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

Instead of assuming we fit models sequentially, assume we have 10 cores (independent processors) that we can fit models on simultaneously. How many minutes would it take?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

:::{.callout-note}
The time it takes to fit each model is hypothetical and does not reflect how long it really took to complete this process.
:::

#### Task 2

We've done a good deal of work fitting many models with various degrees of complexity and computational costs, but we never asked if it was needed and/or worth while for the stated prediction problem. We just assumed it would be. 

Enter baseline models. They are benchmark/reference models that help us frame whether building increasingly complex models would be worth the effort and/or expense.

For our purposes we are interested in 2 baseline models: 

1. the null model and 
2. a very simple linear model. 

These baseline models should fit very quickly, so place both in one script: `3_fit_baseline_ex1.R`. The process for producing results for baseline models follows the familiar tidymodeling steps:

1. Define recipe/preprocessing
2. Define model specification 
    - use `null_model()` with `parsnip` engine
    - use `linear_reg()` with `lm` engine
3. Define workflow
4. Use resamples to train/assess workflow^[Same resamples that all candidate workflows/models should use. Allows for valid comparisons.]

::: {.callout-caution collapse="true" icon="false"}
## Basic baseline recipe/preprocessing 

While it might seem strange, the null model requires a preprocessing step. In this case we will simply use the recipe/preprocessing we plan to use for the other baseline model (a very simple linear model).

For regression problems, linear models with very little preprocessing/feature engineering are good baseline models. They are easy to quickly define, they fit and scale quickly, and their performance is okay. They are also easier to unpack and build from. 

In `2_recipes_ex1.R`, define a new recipe (suggested name `abalone_recipe_baseline`) that starts by using all other variables to predict `age` and includes the following steps:

- Remove `rings`
- Dummy encode all factors
- Remove all variables with zero variance
- Scale and center all numerical variables

:::

When building these baseline models in `3_fit_baseline_ex1.R`, suggest using `null_` and `baseline_` prefixes.

When saving out results of this process, take care to ensure the workflow is included (no need to keep predicted values). This can aid in using built-in analysis tools and ensures we can extract the workflow from the saved results object.

Provide display code for the baseline recipe and the 2 model specification. No need to show anything else --- don't show anything else. Later tasks will provide indications of whether everything was correctly coded. 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 3

Provide a table that displays the best estimated RMSE and standard error for the each model type --- includes baseline models defined in Task 2.

The table information could also be displayed as graph, but is not required.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- no code should be seen or accessible.

:::

Does the table indicate that building more complex models appears to be worth while or not? Explain --- should include the use of both baseline models to explain how you made your decision.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

Which model was the best? This may or may not include identification of the best set of hyperparameters. 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- no code should be seen or accessible.

:::

#### Task 4

Train the best model on the entire training dataset. Assess the model's performance on the test set using RMSE, MAE, MAPE, and $R^2$. Provide an interpretation for each.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- suggets arranging assessment measures in a table for display; no code should be seen or accessible.

:::

As part of the final model assessment process, visualize your results by plotting the predicted observations by the observed observations --- see Figure 9.2 in Tidy Modeling with R.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- no code should be seen or accessible.

:::

### Exercise 2

For this exercise, we will be working with the titanic dataset^[[Kaggle data set](https://www.kaggle.com/c/titanic/overview)]. The dataset contains the a list of 891 passengers that were on the Titanic. 

::: {.callout-note icon="false"}
## Prediction goal

The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).
:::

#### Task 1

::: {.callout-important collapse="true" icon="false"}

## Details for initial setup, model/workflow specs, & preprocessing

When reading in the data we re-typed the `survived` (`"Yes"` was set as its first level/class), `pclass`, and `sex` variables as factors. Analysis of the target variable suggest there where no major issues. There was some mild class imbalance that suggested we should use stratificaion when splitting the data and when forming resamples. We noted there were missingness issues with `cabin`, `age`, and `embarked` that would beed to be addressed during preprocessing. Next, we moved on to splitting the data and constructing resamples.

We implemented an 85-15 training-test split using stratified sampling (stratified by target variable with 4 strata). The resamples were constructed by taking the training dataset and applying repeated V-fold cross-validation (8 folds, 6 repeats) with stratification on the target variable with 4 strata.

The following model types were specified with a plan for tuning hyperparameters using a regular grid:

- logistic regression model (`glm` engine)
- Elastic net model (`glmnet` engine)
    - Mixture was explored over $[0,1]$ range with 21 levels
    - Penalty was explored over $[-3,0]$ range with 21 levels (on log-10 scale)
- K-nearest neighbors model (`kknn` engine)
    - Neighbors was explored over $[1,25]$ with 7 levels
- Random forest (`ranger` engine)
    - Number of trees was explored over $[500, 2000]$ with 4 levels
    - Number of randomly selected predictors to split on was explored over $[1,5]$ with 5 levels
    - Minimum number of data points in a node for splitting was explored over $[2,40]$ with 4 levels
- Boosted tree (`xgboost` engine)
    - Number of trees was explored over $[100, 2000]$ with 6 levels
    - Number of randomly selected predictors to split on was explored over $[1,5]$ with 5 levels
    - Minimum number of data points in a node for splitting was explored over $[2,40]$ with 4 levels
    - Learning rate was explored over $[-5,-0.2]$ with 10 levels (on log-10 scale)
    
The preprocessing/feature engineering for these models can be observed in the `2_recipes_ex2.R` script. Reviewing will help to fully understand the preprocessing steps used.

The logistic and elastic net models used the preprocessing stored in `titanic_recipe_lm`. We take care to remove unneeded variables (`passenger_id`, `name`, `cabin`, `embarked`, `ticket` and variables with zero variance), use linear model to impute missing values for `age`, dummy code predictor variables that are factors, add complexity by including some interactions, and standardize all numerical variables.

The k-nearest neighbors, random forest, and boosted tree models used the preprocessing stored in `titanic_recipe_tree`. We take care to remove unneeded variables (`passenger_id`, `name`, `cabin`, `embarked`, `ticket` and variables with zero variance), use linear model to impute missing values for `age`, one-hot encode predictor variables that are factors, and standardize all numerical variables.

Before any training of models occurs we decided the accuracy would be the metric by which we would compare models.

:::

##### Part A

On each fold, about how much data is used for training the model and about how much is used to produce an assessment estimate? How many assessment estimates are being averaged to produce one estimate per model? 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

##### Part B

It is always useful to have a sense of how many models where included in the model comparison stage --- especially when considering computational resources. Having it broken down by type lets us know which model types were considered and the number of submodels hint at how thorough each model type was explored. Fill in the missing values in @tbl-mod-totals-class.

| Model Type          | Number of models | Total number of trainings^[Number of times a training process must be completed] |
|---------------------|-----------------:|--------------------------:|
| Logistic model      |                  |                           |
| Elastic net         |                  |                           |
| K-nearest neighbors |                  |                           |
| Random forest       |                  |                           |
| Boosted tree        |                  |                           |
| **Total**           |                  |                           |

: Model Training Totals for Classification Problem {#tbl-mod-totals-class .striped .hover}

##### Part C

Suppose each logistic, elastic net, and nearest neighbor model takes 1 second to fit, while each random forest and boosted tree model takes 4 seconds. About how many minutes^[Report in additional unit(s) of time, if it would be more useful/impactful.] would it take to train all the models given the breakdown in @tbl-mod-totals-class --- assuming we fit one model after the other (meaning fit sequentially)? 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

Instead of assuming we fit models sequentially, assume we have 10 cores (independent processors) that we can fit models on simultaneously. How many minutes would it take?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

:::{.callout-note}
The time it takes to fit each model is hypothetical and does not reflect how long it really took to complete this process.
:::

#### Task 2

Similarly to Exercise 1 Task 2, we want know if building increasingly complex models was worth the effort and/or expense. To explore this we will use 2 baseline models:

1. the null model and 
2. a naive Bayes model (common baseline model for binary classification). 

These baseline models should fit very quickly, so place both in one script: `3_fit_baseline_ex2.R`. The process for producing results for baseline models follows the familiar tidymodeling steps:

1. Define recipe/preprocessing
2. Define model specification 
    - use `null_model()` with `parsnip` engine
    - use `naive_Bayes()` with `klaR` engine (note you will need the `discrim` package loaded)
3. Define workflow
4. Use resamples to train/assess workflow^[Same resamples that all candidate workflows/models should use. Allows for valid comparisons.]

::: {.callout-caution collapse="true" icon="false"}
## Basic baseline recipe/preprocessing 

Use the preprocessing/recipe for the tree models as the preprocessing/recipe for the null model. We won't be able to use the naive Bayes preprocessing/recipe due to specific requirement needed for naive Bayes. 

In `2_recipes_ex2.R`, define the naive Bayes recipe (suggested name `titanic_recipe_naive_bayes`) by using all other variables to predict `survived` and includes the following steps:

- Remove `passenger_id`, `name`, `cabin`, `embarked`, and `ticket`
- Remove all variables with zero variance
- Use a linear model to impute missing values for `age`
- Scale and center all numerical variables

:::

When building these baseline models in `3_fit_baseline_ex2.R`, suggest using `null_` and `nbayes_` prefixes.

When saving out results of this process, take care to ensure the workflow is included (no need to keep predicted values). This can aid in using built-in analysis tools and ensures we can extract the workflow from the saved results object.

Provide display code for the baseline recipe and the 2 model specification. No need to show anything else --- don't show anything else. Later tasks will provide indications of whether everything was correctly coded. 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 3

Provide a table that displays the best estimated accuracy and standard error for the each model type --- includes baseline models defined in Task 2.

The table information could also be displayed as graph, but is not required.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- no code should be seen or accessible.

:::


Does the table indicate that building more complex models appears to be worth while or not? Explain --- should include the use of both baseline models to explain how you made your decision.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE 

:::

Which model was the best? This may or may not include identification of the best set of hyperparameters. 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 4

Train the best model on the entire training dataset. Assess the model's performance using the on the test set using accuracy --- provide an interpretation.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- no code should be seen or accessible.

:::

Basic assessment of classification problems must include a confusion matrix. Produce a confusion matrix. Interpret the numbers in each category/cell. The matrix could be displayed as a heat map as well, but is not required.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- no code should be seen or accessible.

:::